#
# Refer to the end of this file to have a look at a sample format of the
# log file that this job is meant to process.
#

# The version of this job configuration file. This should not be changed by the user.
version: 2

# Optional property. Defaults to "false"
#
enabled: false

# Mandatory property.
#
# Source can be of two types:
#   1) file: This allows tailing a log file. It consists of three parameters:
#       - path: Path to the directory where files need to be tailed.
#       - nameGlob: The name/expression to tail the matching files.
#       - startAtEnd: If set to true allows tailing the file from the end.
#         Example:
#         1) path: /var/log
#            nameGlob: */*.log searches for .log files one level deep in the /var/log
#            directory (matches '/var/log/cassandra/system.log' but not
#            '/var/log/apache2/logs/error.log').
#         2) path: /var/log
#            nameGlob: **/*.log searches for .log files even in the sub-directories in
#            /var/log directory (matches both '/var/log/apache2/logs/error.log' and
#            '/var/log/cassandra/system.log').
#       On Windows, path should be provided as if on Unix environments.
#       Ex: demo/logs
#       Ex: C:/app/logs
#   2) syslog: Listens to Syslog messages on a network port. It consists of two parameters:
#       - protocol: The transport layer protocol that will be used. Currently
#         we support only tcp.
#       - port: The port at which the server will be listening to receive the
#         syslog messages. Currently, we support one port per job file, and any
#         enabled job file should be configured to receive messages at different
#         ports. The user must make sure it does not conflict with anything else
#         active in network. If no port number is provided, default port 514
#         will be used.
#
source:
    type: file
    path: ${ANALYTICS_CODEBASE_HOME}/analytics/analytics-agent/src/test/resources
    nameGlob: "*.csv"
    startAtEnd: false

# Optional property (Except "sourceType").
#
# These fields are in addition to the data that is already present in the
# files being tailed. Each record read from the file will be enriched with
# these fields.
#
fields:
   sourceType: csv-log
   nodeName: DataFeed
   tierName: MachineLearning
   appName: Recommendation

# Optional property.
#
# Grok is a way to define and use complex, nested regular expressions in an
# easy to read and use format.
#
# A Grok pattern ultimately resolves and compiles into a regular expression.
# The advantage of using Grok is its ability to compose complex patterns from
# simpler pattern definitions, like a "formal grammar".
#
# See https://grokdebug.herokuapp.com/patterns for examples.
#
# The application comes pre-loaded with some well known Grok patterns in
# the form of ".grok" files. They are available under the "conf/grok" directory.
# Custom Grok files can be added to this directory and they will be
# available for use here when the application is restarted.
#
# The Grok patterns here are meant to match a part of the log "message" string.
# If multiple Grok patterns are provided, each one will be applied to the
# "message" string individually.
#
# A Grok pattern is really a regular expression with the option of referencing
# other known Grok patterns by name. Like this "%{JAVACLASS:myClassName}".
# This means that we are looking for a sub-string that looks like a Java Class
# name. Once the pattern is found, the matching sub-string will be extracted
# and stored separately as a first class field, with "myClassName" as the key.
#
# By default, these patterns do not match multiline strings. To look for
# the pattern sub-string across a multiline string, please refer to:
# http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#DOTALL
#
grok:
  patterns:
    - "%{DATE:quoteDate},%{NUMBER:open},%{NUMBER:high},%{NUMBER:low},%{NUMBER:close},%{NUMBER:volume},%{NUMBER:adjClose}"

# Optional property.
#
# The KeyValue Stage is a way to support parsing of logs to identify and capture
# key-value pairs with a user defined delimiter.
# The KeyValue Stage is defined after the Grok Stage (if defined), and can leverage
# parsing done in the GrokStage to identify the key-value pairs in the logs.
# For details refer to 'sample-glassfish-log.job' file.

# Optional property.
#
# RequestGuid attempts to extract the request GUID from the specified "source" field.
# For details refer to 'sample-analytics-log-with-request-guid.job' file.

# Optional property.
#
# The transform stage is applied after all fields have been captured from the log message.
# The user can specify a list of field names, for which they want to cast
# the value to a specific type or rename the field with an "alias".
# For details refer to 'sample-glassfish-log.job' file.

# ####################################
# ###   Start sample file format   ###
# ####################################
#
# Date,Open,High,Low,Close,Volume,Adj Close
# 2010-05-05,500.98,515.72,500.47,509.76,4566900,509.76
# 2010-05-04,526.52,526.74,504.21,506.37,6076300,506.37
# 2010-05-03,526.50,532.92,525.08,530.60,1857800,530.60
# 2010-04-30,531.13,537.68,525.44,525.70,2435400,525.70
# 2010-04-29,533.37,536.50,526.67,532.00,3058900,532.00
# 2010-04-28,532.10,534.83,521.03,529.19,3406100,529.19
# 2010-04-27,528.95,538.33,527.23,529.06,3844700,529.06
# 2010-04-26,544.97,544.99,529.21,531.64,4368800,531.64
# 2010-04-23,547.25,549.32,542.27,544.99,2089400,544.99
#
# ##################################
# ###   End sample file format   ###
# ##################################
